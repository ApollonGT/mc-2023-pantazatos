{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2759eab1-23da-4a36-bec4-454d8b55ebe5",
   "metadata": {},
   "source": [
    "Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e69e3-86df-4d50-917f-d79321abae3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio \n",
    "!pip install transformers\n",
    "!pip install sklearn\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install tabulate\n",
    "!pip install tqdm\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8923fc1-f310-4213-a8a2-82b204beb61e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import XLNetForSequenceClassification, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e3f79c-069f-4f4d-8544-699cc0847b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('IMDB_Reviews_Top_250_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf2c4a-8f22-495d-9a2c-45b095f9cf98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8a455-7b73-479f-aa94-abe5a7bf7610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the threshold for binary conversion\n",
    "threshold = 7.0\n",
    "\n",
    "# Convert the 'Rating' column to a binary variable\n",
    "data['Binary Rating'] = data['Rating'].apply(lambda x: 1 if x >= threshold else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800ffc78-45d9-4b38-9703-812a8be712a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract reviews texts and ratings into arrays\n",
    "reviews = data['Review Text'].values\n",
    "labels = data['Binary Rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8103b78-100b-44a6-ab5d-6deb00a55924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shouldn't be necessary but just to be safe\n",
    "for review in reviews:\n",
    "    str(review)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d3578-74c5-4d02-b6df-0a5156f55899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count the number of rows with label 1\n",
    "num_label_1 = data[data['Binary Rating'] == 1]['Binary Rating'].count()\n",
    "\n",
    "# Count the number of rows with label 0\n",
    "num_label_0 = data[data['Binary Rating'] == 0]['Binary Rating'].count()\n",
    "\n",
    "print(f\"Number of rows with Binary Rating 1: {num_label_1}\")\n",
    "print(f\"Number of rows with Binary Rating 0: {num_label_0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4c1c0-91e7-447e-9e1a-9cba398c17b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reviews[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8461987-9431-496e-8b9d-b268e2919fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c7c82-7bb7-49fe-83d9-c629f8e5f266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load XLNet tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddd57b5-90ac-4d7d-a9d9-c3ccc9c73af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing tokens and token Ids for a random sentence\n",
    "def print_rand_sentence():\n",
    "    index = random.randint(0, len(reviews)-1) #random index in texts list\n",
    "    table = np.array([tokenizer.tokenize(reviews[index]), \n",
    "                    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(reviews[index]))]).T #tokenize random text in texts list\n",
    "    print(tabulate(table,\n",
    "                 headers = ['Tokens', 'Token IDs'],\n",
    "                 tablefmt = 'fancy_grid'))  #print in table format\n",
    "\n",
    "print_rand_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8f729-5bf5-4891-b62c-c102e2d19ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find max sequensce lenght\n",
    "\n",
    "MAX_LEN = 0\n",
    "for review in reviews:\n",
    "    tokenized = tokenizer(review,return_tensors='pt',add_special_tokens=True)\n",
    "    MAX_LEN = max(MAX_LEN, tokenized['input_ids'].size()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef964e35-aaac-4cea-9bc5-538bd464307d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10053f5e-2086-4231-93cc-b52d449cfa3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize and encode texts then extract token ids, attention masks and labels in torch tensor format.\n",
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "def preprocessing(input_text, tokenizer):\n",
    "    return tokenizer.encode_plus(                    #returns dictionary with token ids, attention masks and token type ids\n",
    "                        input_text,\n",
    "                        max_length = 512,\n",
    "                        add_special_tokens = True,\n",
    "                        padding = 'max_length',    #padding tokens to be of the same size\n",
    "                        truncation=True,           # Truncate the text if it exceeds 512 tokens\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'        #torch tensor format\n",
    "                   )\n",
    "\n",
    "\n",
    "for sample in reviews:\n",
    "    encoding_dict = preprocessing(sample, tokenizer)\n",
    "    token_id.append(encoding_dict['input_ids']) \n",
    "    attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "#concatinating the tesnors in a single dimension\n",
    "token_id = torch.cat(token_id, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf64705-2553-4473-b355-5e5322520b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c01630e-0bbe-44c2-86e3-e6cc35051253",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f7c03-c384-4e46-8de5-2f2cfd8b346c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081dd9a8-0f22-468c-bf6a-bf16eb4193e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print ids and masks for a random sentence\n",
    "def print_rand_sentence_encoding():\n",
    "    index = random.randint(0, len(review) - 1)\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n",
    "    token_ids = [i.numpy() for i in token_id[index]]\n",
    "    attention = [i.numpy() for i in attention_masks[index]]\n",
    "    table = np.array([tokens, token_ids, attention]).T\n",
    "    print(tabulate(table, \n",
    "                 headers = ['Tokens', 'Token IDs', 'Attention Mask'],\n",
    "                 tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f0a72-6555-4e87-8f4e-9088d52490fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split data in training and validation sets\n",
    "\n",
    "val_ratio = 0.2 \n",
    "batch_size = 16\n",
    "\n",
    "# Indices of the train and validation splits stratified by labels\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(labels)),\n",
    "    test_size = val_ratio,\n",
    "    shuffle = True,\n",
    "    stratify = labels)\n",
    "\n",
    "# Train and validation sets\n",
    "train_set = TensorDataset(token_id[train_idx], \n",
    "                          attention_masks[train_idx], \n",
    "                          labels[train_idx])\n",
    "\n",
    "val_set = TensorDataset(token_id[val_idx], \n",
    "                        attention_masks[val_idx], \n",
    "                        labels[val_idx])\n",
    "\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "            train_set,\n",
    "            sampler = RandomSampler(train_set),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_set,\n",
    "            sampler = SequentialSampler(val_set),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b5a03-dc25-4571-bc44-2eabac746cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = XLNetForSequenceClassification.from_pretrained(\n",
    "    \"xlnet-base-cased\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr=3e-5,     # You can also try 3e-5, 2e-5\n",
    "                eps=1e-8     # AdamW's epsilon value. Probably optimal.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc48476-ba84-4f58-8d65-f145d3d89ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define device and move the model to it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b78c929-6c0b-4430-ac34-39f7acda775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "debug_interval = 60  # Print a debug message every 60 seconds\n",
    "\n",
    "# Define variables for early stopping\n",
    "patience = 3\n",
    "min_delta = 0.001\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "start_time = time.time()  # Record the start time\n",
    "\n",
    "for epoch in trange(epochs, desc='Epoch'):\n",
    "    # Set model to training mode for training loop\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        train_output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Backward pass\n",
    "        train_output.loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += train_output.loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        # Check if five seconds have passed, and print a debug message\n",
    "        if time.time() - start_time > debug_interval:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Step {step + 1}/{len(train_dataloader)}, \"\n",
    "                  f\"Train Loss: {tr_loss / nb_tr_steps:.4f}\")\n",
    "            start_time = time.time()  # Reset the start time for the next 5 seconds\n",
    "\n",
    "    # Set model to evaluation mode for validation loop\n",
    "    model.eval()\n",
    "        \n",
    "    # Set model to evaluation mode for validation loop\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialise metrics \n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            eval_output = model(b_input_ids, \n",
    "                              token_type_ids=None, \n",
    "                              attention_mask=b_input_mask, \n",
    "                              labels=b_labels)\n",
    "        logits = eval_output.logits\n",
    "        eval_loss = eval_output.loss\n",
    "        val_loss += eval_loss.item()\n",
    "\n",
    "        # Calculate validation metrics\n",
    "        preds = torch.argmax(logits, dim=1).cpu().detach().numpy()\n",
    "        label_ids = b_labels.cpu().detach().numpy()\n",
    "        val_preds.extend(preds)\n",
    "        val_labels.extend(label_ids)\n",
    "\n",
    "    # Calculate average metrics over all batches\n",
    "    avg_val_loss = val_loss / len(validation_dataloader)\n",
    "    avg_val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "    avg_val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "\n",
    "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
    "    print('\\n\\t - Validation loss: {:.4f}'.format(avg_val_loss))\n",
    "    print('\\n\\t - Validation accuracy: {:.4f}'.format(avg_val_accuracy))\n",
    "    print('\\n\\t - Validation F1 score: {:.4f}'.format(avg_val_f1))\n",
    "\n",
    "    # Check if validation loss improved\n",
    "    if avg_val_loss < best_val_loss - min_delta * best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        # Save the model\n",
    "        torch.save(model.state_dict(), 'best_model_xlnet.pt')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter == patience:\n",
    "            print(\"Validation loss did not improve for {} epochs. Early stopping...\".format(patience))\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
